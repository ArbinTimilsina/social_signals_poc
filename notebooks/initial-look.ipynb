{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4cdde16b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "# Weights for Social Signals rank\n",
    "SUBMISSION_WEIGHT = 0.35\n",
    "COMMENT_WEIGHT = 0.65\n",
    "\n",
    "# For submission data\n",
    "SUBMISSION_TIME_FILTER = \"day\"\n",
    "SUBMISSION_LIMIT = 15\n",
    "\n",
    "# For comments data\n",
    "COMMENT_SORT = \"top\"\n",
    "COMMENT_LIMIT = 15\n",
    "\n",
    "# For DB\n",
    "SCHEMA = \"social_signals_dev\"\n",
    "TABLE_NAME = \"social_signals_poc\"\n",
    "\n",
    "CLASSIFICATION_THRESHOLD = 0.75\n",
    "NONE_FILLER = \"000\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3f71097b",
   "metadata": {},
   "outputs": [],
   "source": [
    "HUGGINGFACE_TOKEN = os.getenv(\"huggingface_token\")\n",
    "\n",
    "NER_MODEL_ID = \"dslim/bert-large-NER\"\n",
    "EMOTION_MODEL_ID = \"j-hartmann/emotion-english-distilroberta-base\"\n",
    "ESG_CATEGORIES_MODEL_ID = \"yiyanghkust/finbert-esg-9-categories\"\n",
    "\n",
    "\n",
    "def get_huggingface_response(text, model_id):\n",
    "    api_url = f\"https://api-inference.huggingface.co/models/{model_id}\"\n",
    "    headers = {\"Authorization\": f\"Bearer {HUGGINGFACE_TOKEN}\"}\n",
    "\n",
    "    payload = {\"inputs\": text, \"options\": {\"wait_for_model\": True}}\n",
    "    response = requests.post(api_url, headers=headers, json=payload)\n",
    "\n",
    "    try:\n",
    "        response = response.json()\n",
    "    except Exception:\n",
    "        print(f\"Could not get Huggingface response for {model_id}\")\n",
    "        return\n",
    "    return response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1acb1209",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "NONE_FILLER = \"000\"\n",
    "\n",
    "openai.api_key = os.getenv(\"openai_key\")\n",
    "\n",
    "\n",
    "def get_openai_summary(text):\n",
    "    prompt = f\"{text} \\n\\nTl;dr\"\n",
    "    \n",
    "    try:\n",
    "        response = openai.Completion.create(\n",
    "            model=\"text-davinci-003\",\n",
    "            prompt=prompt,\n",
    "            temperature=0.7,\n",
    "            max_tokens=60,\n",
    "            top_p=1.0,\n",
    "            frequency_penalty=0.0,\n",
    "            presence_penalty=1,\n",
    "        )\n",
    "    except Exception:\n",
    "        print(\"Could not get OpenAI response\")\n",
    "        return NONE_FILLER\n",
    "\n",
    "    return response[\"choices\"][0][\"text\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1ad21184",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "G1dhM7-gSCPe5qQ7fOFSUQ\n"
     ]
    }
   ],
   "source": [
    "import praw\n",
    "\n",
    "REDDIT_ID = os.getenv(\"reddit_id\")\n",
    "print(REDDIT_ID)\n",
    "REDDIT_SECRET = os.getenv(\"reddit_secret\")\n",
    "REDDIT_USERNAME = os.getenv(\"reddit_username\")\n",
    "REDDIT_PASSWORD = os.getenv(\"reddit_password\")\n",
    "\n",
    "\n",
    "def get_reddit():\n",
    "    reddit = praw.Reddit(\n",
    "        user_agent=\"SocialSignals/1.0\",\n",
    "        client_id=REDDIT_ID,\n",
    "        client_secret=REDDIT_SECRET,\n",
    "        username=REDDIT_USERNAME,\n",
    "        password=REDDIT_PASSWORD,\n",
    "    )\n",
    "    return reddit\n",
    "\n",
    "\n",
    "def get_subreddit(subreddit_name):\n",
    "    reddit = get_reddit()\n",
    "    subreddit = reddit.subreddit(subreddit_name)\n",
    "\n",
    "    return subreddit\n",
    "\n",
    "\n",
    "def get_top_submissions(subreddit_name, time_filter=\"day\", limit=10):\n",
    "    \"\"\"\n",
    "    time_filter: Can be one of: \"all\", \"day\", \"hour\", \"month\", \"week\", or \"year\"\n",
    "    \"\"\"\n",
    "    subreddit = get_subreddit(subreddit_name)\n",
    "    top_submissions = subreddit.top(time_filter=time_filter, limit=limit)\n",
    "\n",
    "    return subreddit, top_submissions\n",
    "\n",
    "\n",
    "def get_comments(submission, comment_sort=\"top\", comment_limit=10):\n",
    "    \"\"\"\n",
    "    comment_sort: Can be one of: \"confidence\", \"controversial\", \"new\", \"old\", \"q&a\", and \"top\"\n",
    "    \"\"\"\n",
    "    # Calling replace_more() access comments, and so must be done after comment_sort is updated\n",
    "    submission.comment_sort = comment_sort\n",
    "    submission.comment_limit = comment_limit\n",
    "\n",
    "    # Remove comments like \"load more comments”, and “continue this thread”\n",
    "    submission.comments.replace_more(limit=0)\n",
    "\n",
    "    comments = submission.comments\n",
    "    return comments\n",
    "\n",
    "\n",
    "def get_submission_data(subreddit, submission):\n",
    "    submission_data = {}\n",
    "\n",
    "    subreddit_name = subreddit.display_name\n",
    "    submission_data[\"subreddit_name\"] = subreddit_name\n",
    "\n",
    "    submission_id = submission.id\n",
    "    submission_data[\"submission_id\"] = submission_id\n",
    "\n",
    "    submission_title = submission.title\n",
    "    submission_data[\"submission_title\"] = submission_title\n",
    "    print(f\"Submission title: {submission_title}\")\n",
    "\n",
    "    print(\"Getting entities for the title...\")\n",
    "    huggingface_entities = get_huggingface_response(submission_title, NER_MODEL_ID)\n",
    "    organization, person, location = [], [], []\n",
    "    if isinstance(huggingface_entities, list):\n",
    "        for entity in huggingface_entities:\n",
    "            if (\n",
    "                entity[\"entity_group\"] == \"ORG\"\n",
    "                and entity[\"score\"] >= CLASSIFICATION_THRESHOLD\n",
    "            ):\n",
    "                organization.append(entity[\"word\"])\n",
    "            if (\n",
    "                entity[\"entity_group\"] == \"PER\"\n",
    "                and entity[\"score\"] >= CLASSIFICATION_THRESHOLD\n",
    "            ):\n",
    "                person.append(entity[\"word\"])\n",
    "            if (\n",
    "                entity[\"entity_group\"] == \"LOC\"\n",
    "                and entity[\"score\"] >= CLASSIFICATION_THRESHOLD\n",
    "            ):\n",
    "                location.append(entity[\"word\"])\n",
    "\n",
    "    if not (organization + person + location):\n",
    "        print(\"Expected entities not found. Quitting...\")\n",
    "        return\n",
    "\n",
    "    if organization:\n",
    "        submission_data[\"organization\"] = \", \".join(organization)\n",
    "    else:\n",
    "        submission_data[\"organization\"] = NONE_FILLER\n",
    "    if person:\n",
    "        submission_data[\"person\"] = \", \".join(person)\n",
    "    else:\n",
    "        submission_data[\"person\"] = NONE_FILLER\n",
    "    if location:\n",
    "        submission_data[\"location\"] = \", \".join(location)\n",
    "    else:\n",
    "        submission_data[\"location\"] = NONE_FILLER\n",
    "\n",
    "    subreddit_subscribers = subreddit.subscribers\n",
    "    submission_data[\"subreddit_subscribers\"] = subreddit_subscribers\n",
    "\n",
    "    submission_score = submission.score\n",
    "    submission_data[\"submission_score\"] = submission_score\n",
    "\n",
    "    submission_num_comments = submission.num_comments\n",
    "    submission_data[\"submission_num_comments\"] = submission_num_comments\n",
    "\n",
    "    return submission_data\n",
    "\n",
    "\n",
    "def process_submission_data(\n",
    "    submission_id, submission_title, comment_sort=\"top\", comment_limit=10\n",
    "):\n",
    "    reddit = get_reddit()\n",
    "    submission = reddit.submission(submission_id)\n",
    "\n",
    "    assert submission.title == submission_title, \"Miss-match in submission title!\"\n",
    "\n",
    "    submission_data = {}\n",
    "\n",
    "    print(\"Getting emotion for the title...\")\n",
    "    title_emotion = get_huggingface_response(submission_title, EMOTION_MODEL_ID)\n",
    "    if isinstance(title_emotion, list):\n",
    "        title_emotion_prediction = title_emotion[0][0][\"label\"]\n",
    "        title_emotion_score = title_emotion[0][0][\"score\"]\n",
    "        if title_emotion_score >= CLASSIFICATION_THRESHOLD:\n",
    "            submission_data[\"title_emotion\"] = title_emotion_prediction\n",
    "        else:\n",
    "            submission_data[\"title_emotion\"] = \"neutral\"\n",
    "\n",
    "    print(\"Getting ESG categories for the title...\")\n",
    "    title_esg_categories = get_huggingface_response(\n",
    "        submission_title, ESG_CATEGORIES_MODEL_ID\n",
    "    )\n",
    "    if isinstance(title_esg_categories, list):\n",
    "        title_esg_categories_prediction = title_esg_categories[0][0][\"label\"]\n",
    "        title_esg_categories_score = title_esg_categories[0][0][\"score\"]\n",
    "        if title_esg_categories_score:\n",
    "            submission_data[\"categories\"] = title_esg_categories_prediction\n",
    "\n",
    "    print(\"Going over comments...\")\n",
    "    top_level_comments = get_comments(\n",
    "        submission=submission, comment_sort=comment_sort, comment_limit=comment_limit\n",
    "    )\n",
    "    comments_emotion_counter, comments = {}, []\n",
    "    for top_level_comment in top_level_comments:\n",
    "        comment = top_level_comment.body\n",
    "        comments.append(comment)\n",
    "        print(f\"comment: {comment}\")\n",
    "\n",
    "        comment_emotion = get_huggingface_response(comment, EMOTION_MODEL_ID)\n",
    "\n",
    "        if isinstance(comment_emotion, list):\n",
    "            comment_emotion_prediction = comment_emotion[0][0][\"label\"]\n",
    "            comment_emotion_score = comment_emotion[0][0][\"score\"]\n",
    "            comment_emotion_score = round(comment_emotion_score, 2)\n",
    "            if comment_emotion_score >= CLASSIFICATION_THRESHOLD:\n",
    "                comments_emotion_counter[comment_emotion_prediction] = (\n",
    "                    comments_emotion_counter.get(comment_emotion_prediction, 0) + 1\n",
    "                )\n",
    "    if comments_emotion_counter:\n",
    "        submission_data[\"comments_emotion\"] = max(\n",
    "            comments_emotion_counter, key=comments_emotion_counter.get\n",
    "        )\n",
    "    else:\n",
    "        submission_data[\"comments_emotion\"] = NONE_FILLER\n",
    "\n",
    "    if comments:\n",
    "        summary_text = \" \".join(comments)\n",
    "        summary = get_openai_summary(summary_text)\n",
    "\n",
    "        submission_data[\"comments_summary\"] = summary\n",
    "    else:\n",
    "        submission_data[\"comments_summary\"] = NONE_FILLER\n",
    "\n",
    "    return submission_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eab5202f-afeb-4d83-84d2-09018903c26a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the combined df is (103, 12)\n",
      "Processing entity organization\n",
      "Shape of the df is (48, 12)\n",
      "Getting emotion for the title...\n",
      "Getting ESG categories for the title...\n",
      "Going over comments...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/social-signals-env/lib/python3.8/site-packages/praw/models/reddit/submission.py:607: UserWarning: The comments for this submission have already been fetched, so the updated comment_sort will not have any effect\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting emotion for the title...\n",
      "Getting ESG categories for the title...\n",
      "Going over comments...\n",
      "Could not get OpenAI response\n",
      "Getting emotion for the title...\n",
      "Getting ESG categories for the title...\n",
      "Going over comments...\n",
      "Getting emotion for the title...\n",
      "Getting ESG categories for the title...\n",
      "Going over comments...\n",
      "Writing df of shape (1, 11) to the DB\n",
      "Writing df of shape (1, 11) to the DB\n",
      "Writing df of shape (1, 11) to the DB\n",
      "Processing entity person\n",
      "Shape of the df is (68, 12)\n",
      "Getting emotion for the title...\n",
      "Getting ESG categories for the title...\n",
      "Going over comments...\n",
      "Could not get OpenAI response\n",
      "Getting emotion for the title...\n",
      "Getting ESG categories for the title...\n",
      "Going over comments...\n",
      "Getting emotion for the title...\n",
      "Getting ESG categories for the title...\n",
      "Going over comments...\n",
      "Could not get OpenAI response\n",
      "Getting emotion for the title...\n",
      "Getting ESG categories for the title...\n",
      "Going over comments...\n",
      "Could not get OpenAI response\n",
      "Getting emotion for the title...\n",
      "Getting ESG categories for the title...\n",
      "Going over comments...\n",
      "Could not get OpenAI response\n",
      "Getting emotion for the title...\n",
      "Getting ESG categories for the title...\n",
      "Going over comments...\n",
      "Getting emotion for the title...\n",
      "Getting ESG categories for the title...\n",
      "Going over comments...\n",
      "Could not get OpenAI response\n",
      "Getting emotion for the title...\n",
      "Getting ESG categories for the title...\n",
      "Going over comments...\n",
      "Getting emotion for the title...\n",
      "Getting ESG categories for the title...\n",
      "Going over comments...\n",
      "Writing df of shape (1, 11) to the DB\n",
      "Writing df of shape (1, 11) to the DB\n",
      "Writing df of shape (1, 11) to the DB\n",
      "Processing entity location\n",
      "Shape of the df is (43, 12)\n",
      "Getting emotion for the title...\n",
      "Getting ESG categories for the title...\n",
      "Going over comments...\n",
      "Could not get OpenAI response\n",
      "Getting emotion for the title...\n",
      "Getting ESG categories for the title...\n",
      "Going over comments...\n",
      "Could not get OpenAI response\n",
      "Getting emotion for the title...\n",
      "Getting ESG categories for the title...\n",
      "Going over comments...\n",
      "Getting emotion for the title...\n",
      "Getting ESG categories for the title...\n",
      "Going over comments...\n",
      "Getting emotion for the title...\n",
      "Getting ESG categories for the title...\n",
      "Going over comments...\n",
      "Could not get OpenAI response\n",
      "Getting emotion for the title...\n",
      "Getting ESG categories for the title...\n",
      "Going over comments...\n",
      "Getting emotion for the title...\n",
      "Getting ESG categories for the title...\n",
      "Going over comments...\n",
      "Could not get OpenAI response\n",
      "Getting emotion for the title...\n",
      "Getting ESG categories for the title...\n",
      "Going over comments...\n",
      "Writing df of shape (1, 11) to the DB\n",
      "Writing df of shape (1, 11) to the DB\n",
      "Writing df of shape (1, 11) to the DB\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "\n",
    "def get_submission_data2(year, month, day, df, submission_ids, entity, top_n=3):\n",
    "    print(f\"Processing entity {entity}\")\n",
    "    df_entity = df[df[entity] != NONE_FILLER]\n",
    "    print(f\"Shape of the df is {df_entity.shape}\")\n",
    "\n",
    "    submission_data_list = []\n",
    "    count = 0\n",
    "    for _, row in df_entity.iterrows():\n",
    "        submission_id = row[\"submission_id\"]\n",
    "        submission_title = row[\"submission_title\"]\n",
    "        submission_data = process_submission_data(\n",
    "            submission_id=submission_id,\n",
    "            submission_title=submission_title,\n",
    "            comment_sort=COMMENT_SORT,\n",
    "            comment_limit=COMMENT_LIMIT,\n",
    "        )\n",
    "        comments_summary = submission_data[\"comments_summary\"]\n",
    "        if comments_summary == NONE_FILLER:\n",
    "            continue\n",
    "        if submission_id in submission_ids:\n",
    "            continue\n",
    "        submission_ids.append(submission_id)\n",
    "\n",
    "        submission_data[\"bucket\"] = entity.capitalize()\n",
    "        submission_data[\"year\"] = year\n",
    "        submission_data[\"day\"] = day\n",
    "        submission_data[\"month\"] = month\n",
    "        submission_data[\"title\"] = submission_title\n",
    "\n",
    "        subreddit_name = row[\"subreddit_name\"]\n",
    "        submission_data[\"source\"] = f\"reddit.com/r/{subreddit_name}/{submission_id}\"\n",
    "\n",
    "        entities = row[entity]\n",
    "        submission_data[\"tags\"] = entities\n",
    "\n",
    "        submission_data_list.append(submission_data)\n",
    "        count += 1\n",
    "        if count == top_n:\n",
    "            break\n",
    "    return submission_data_list\n",
    "\n",
    "year = \"2023\"\n",
    "month = \"05\"\n",
    "day = \"23\"\n",
    "\n",
    "input_path = f\"s3://social-signals-dev-data/reddit/year={year}/month={month}/day={day}/combined.csv\"\n",
    "df = pd.read_csv(input_path)\n",
    "print(f\"Shape of the combined df is {df.shape}\")\n",
    "\n",
    "entities = [\"location\"]\n",
    "submission_ids = []\n",
    "for entity in entities:        \n",
    "    submission_data_list = get_submission_data2(year, month, day, df, submission_ids, entity=entity)\n",
    "\n",
    "    for submission_data in submission_data_list:\n",
    "        db_df = pd.DataFrame(data=[submission_data])\n",
    "        print(f\"Writing df of shape {db_df.shape} to the DB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4bf02ebc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subreddit_name</th>\n",
       "      <th>submission_id</th>\n",
       "      <th>submission_title</th>\n",
       "      <th>organization</th>\n",
       "      <th>person</th>\n",
       "      <th>location</th>\n",
       "      <th>subreddit_subscribers</th>\n",
       "      <th>submission_score</th>\n",
       "      <th>submission_num_comments</th>\n",
       "      <th>submission_rank</th>\n",
       "      <th>comment_rank</th>\n",
       "      <th>social_signals_rank</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>movies</td>\n",
       "      <td>13pugz8</td>\n",
       "      <td>Hi, I’m Bert Kreischer, stand-up comedian and ...</td>\n",
       "      <td>000</td>\n",
       "      <td>Bert Kreischer</td>\n",
       "      <td>000</td>\n",
       "      <td>30954285</td>\n",
       "      <td>106</td>\n",
       "      <td>579</td>\n",
       "      <td>0.000412</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.650144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>entertainment</td>\n",
       "      <td>13pp69j</td>\n",
       "      <td>Matt Damon Calls Off ‘Oppenheimer’ vs. ‘Barbie...</td>\n",
       "      <td>000</td>\n",
       "      <td>Matt Damon</td>\n",
       "      <td>000</td>\n",
       "      <td>3891380</td>\n",
       "      <td>31397</td>\n",
       "      <td>1317</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.007679</td>\n",
       "      <td>0.354992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>politics</td>\n",
       "      <td>13plzp2</td>\n",
       "      <td>Burger bar sues Ron DeSantis over ban on \"adul...</td>\n",
       "      <td>000</td>\n",
       "      <td>Burger, Ron DeSantis</td>\n",
       "      <td>000</td>\n",
       "      <td>8326450</td>\n",
       "      <td>34122</td>\n",
       "      <td>2009</td>\n",
       "      <td>0.507908</td>\n",
       "      <td>0.010779</td>\n",
       "      <td>0.184774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>politics</td>\n",
       "      <td>13pvmwv</td>\n",
       "      <td>Rick Scott issues travel advisory for ‘sociali...</td>\n",
       "      <td>000</td>\n",
       "      <td>Rick Scott</td>\n",
       "      <td>Florida</td>\n",
       "      <td>8326450</td>\n",
       "      <td>25201</td>\n",
       "      <td>2890</td>\n",
       "      <td>0.375115</td>\n",
       "      <td>0.020995</td>\n",
       "      <td>0.144937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>politics</td>\n",
       "      <td>13q5bzz</td>\n",
       "      <td>Twitter Is a Far-Right Social Network. It can ...</td>\n",
       "      <td>Twitter</td>\n",
       "      <td>000</td>\n",
       "      <td>000</td>\n",
       "      <td>8326450</td>\n",
       "      <td>22180</td>\n",
       "      <td>2341</td>\n",
       "      <td>0.330146</td>\n",
       "      <td>0.019323</td>\n",
       "      <td>0.128111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>worldnews</td>\n",
       "      <td>13pwd7h</td>\n",
       "      <td>A group of activists said it has mapped more t...</td>\n",
       "      <td>000</td>\n",
       "      <td>000</td>\n",
       "      <td>Russia, Moscow</td>\n",
       "      <td>31782214</td>\n",
       "      <td>2318</td>\n",
       "      <td>19</td>\n",
       "      <td>0.009028</td>\n",
       "      <td>0.001501</td>\n",
       "      <td>0.004135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>UpliftingNews</td>\n",
       "      <td>13ppm5p</td>\n",
       "      <td>City of Oakville Canada makes bus rides free f...</td>\n",
       "      <td>City of Oakville Canada</td>\n",
       "      <td>0</td>\n",
       "      <td>000</td>\n",
       "      <td>19039454</td>\n",
       "      <td>378</td>\n",
       "      <td>10</td>\n",
       "      <td>0.002449</td>\n",
       "      <td>0.004843</td>\n",
       "      <td>0.004005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>sports</td>\n",
       "      <td>13pjz5f</td>\n",
       "      <td>Four held in Spain over Vinicius Jr effigy han...</td>\n",
       "      <td>000</td>\n",
       "      <td>Vinicius Jr</td>\n",
       "      <td>Spain</td>\n",
       "      <td>20640459</td>\n",
       "      <td>69</td>\n",
       "      <td>2</td>\n",
       "      <td>0.000402</td>\n",
       "      <td>0.005307</td>\n",
       "      <td>0.003590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>UpliftingNews</td>\n",
       "      <td>13pr6zb</td>\n",
       "      <td>How India became a frontrunner in the global r...</td>\n",
       "      <td>000</td>\n",
       "      <td>0</td>\n",
       "      <td>India</td>\n",
       "      <td>19039454</td>\n",
       "      <td>57</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000359</td>\n",
       "      <td>0.003212</td>\n",
       "      <td>0.002213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>sports</td>\n",
       "      <td>13piuvs</td>\n",
       "      <td>Wests Tigers reach 66 points for the first tim...</td>\n",
       "      <td>Wests Tigers</td>\n",
       "      <td>000</td>\n",
       "      <td>000</td>\n",
       "      <td>20640459</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>103 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    subreddit_name submission_id   \n",
       "0           movies       13pugz8  \\\n",
       "1    entertainment       13pp69j   \n",
       "2         politics       13plzp2   \n",
       "3         politics       13pvmwv   \n",
       "4         politics       13q5bzz   \n",
       "..             ...           ...   \n",
       "98       worldnews       13pwd7h   \n",
       "99   UpliftingNews       13ppm5p   \n",
       "100         sports       13pjz5f   \n",
       "101  UpliftingNews       13pr6zb   \n",
       "102         sports       13piuvs   \n",
       "\n",
       "                                      submission_title   \n",
       "0    Hi, I’m Bert Kreischer, stand-up comedian and ...  \\\n",
       "1    Matt Damon Calls Off ‘Oppenheimer’ vs. ‘Barbie...   \n",
       "2    Burger bar sues Ron DeSantis over ban on \"adul...   \n",
       "3    Rick Scott issues travel advisory for ‘sociali...   \n",
       "4    Twitter Is a Far-Right Social Network. It can ...   \n",
       "..                                                 ...   \n",
       "98   A group of activists said it has mapped more t...   \n",
       "99   City of Oakville Canada makes bus rides free f...   \n",
       "100  Four held in Spain over Vinicius Jr effigy han...   \n",
       "101  How India became a frontrunner in the global r...   \n",
       "102  Wests Tigers reach 66 points for the first tim...   \n",
       "\n",
       "                organization                person        location   \n",
       "0                        000        Bert Kreischer             000  \\\n",
       "1                        000            Matt Damon             000   \n",
       "2                        000  Burger, Ron DeSantis             000   \n",
       "3                        000            Rick Scott         Florida   \n",
       "4                    Twitter                   000             000   \n",
       "..                       ...                   ...             ...   \n",
       "98                       000                   000  Russia, Moscow   \n",
       "99   City of Oakville Canada                     0             000   \n",
       "100                      000           Vinicius Jr           Spain   \n",
       "101                      000                     0           India   \n",
       "102             Wests Tigers                   000             000   \n",
       "\n",
       "     subreddit_subscribers  submission_score  submission_num_comments   \n",
       "0                 30954285               106                      579  \\\n",
       "1                  3891380             31397                     1317   \n",
       "2                  8326450             34122                     2009   \n",
       "3                  8326450             25201                     2890   \n",
       "4                  8326450             22180                     2341   \n",
       "..                     ...               ...                      ...   \n",
       "98                31782214              2318                       19   \n",
       "99                19039454               378                       10   \n",
       "100               20640459                69                        2   \n",
       "101               19039454                57                        1   \n",
       "102               20640459                 2                        0   \n",
       "\n",
       "     submission_rank  comment_rank  social_signals_rank  \n",
       "0           0.000412      1.000000             0.650144  \n",
       "1           1.000000      0.007679             0.354992  \n",
       "2           0.507908      0.010779             0.184774  \n",
       "3           0.375115      0.020995             0.144937  \n",
       "4           0.330146      0.019323             0.128111  \n",
       "..               ...           ...                  ...  \n",
       "98          0.009028      0.001501             0.004135  \n",
       "99          0.002449      0.004843             0.004005  \n",
       "100         0.000402      0.005307             0.003590  \n",
       "101         0.000359      0.003212             0.002213  \n",
       "102         0.000000      0.000000             0.000000  \n",
       "\n",
       "[103 rows x 12 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e4ab6620",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-03-22-15-17-20\n",
      "2017-03-22\n",
      "2017 03 22 00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/6k/s74xljp501lc_129l1h7nwjh0000gp/T/ipykernel_28059/623717244.py:4: FutureWarning: The behavior of 'to_datetime' with 'unit' when parsing strings is deprecated. In a future version, strings will be parsed as datetime strings, matching the behavior without a 'unit'. To retain the old behavior, explicitly cast ints or floats to numeric type before calling to_datetime.\n",
      "  execution_date_str = pd.to_datetime(\"1490195805\", unit='s').strftime(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "execution_date_str = pd.to_datetime(\"1490195805\", unit='s').strftime(\n",
    "        \"%Y-%m-%d-%H-%M-%S\"\n",
    ")\n",
    "print(execution_date_str)\n",
    "given_date = datetime.strptime(execution_date_str, \"%Y-%m-%d-%H-%M-%S\").date()\n",
    "print(given_date)\n",
    "year = given_date.strftime(\"%Y\")\n",
    "month = given_date.strftime(\"%m\")\n",
    "day = given_date.strftime(\"%d\")\n",
    "time = given_date.strftime(\"%H\")\n",
    "print(year, month, day, time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bae19a2f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "social-signals-env",
   "language": "python",
   "name": "social-signals-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
